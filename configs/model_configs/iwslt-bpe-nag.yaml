model_configs:
  # model basic configs
  model_select: CondAttNAG
  task_type: Translation
  src_max_time_step: 200
  tgt_max_time_step: 200
  cuda: true
  eval_mode: bleu
  eval_bs: 20
  dev_item: BLEU
  warm_up: -1
  # encoder configs
  embed_size: &nes 256
  hidden_size: &nhs 256
  inner_size: &nis 512
  rnn_type: lstm
  enc_type: att # [rnn,att]
  enc_embed_dim: *nes
  enc_hidden_dim: *nhs
  enc_ed: &ned 0.1
  enc_rd: &nrd 0.1
  bidirectional: True
  enc_head: 2
  enc_num_layers: 5
  enc_inner_hidden: *nis
  # decoder configs
  dec_embed_dim: *nes
  dec_num_layers: 5
  dec_head: 2
  dec_hidden_dim: *nhs
  dec_inner_hidden: *nis
  dec_rd: *nrd
  dropm: *ned
  dropo: *nrd
  use_arc: false
  use_dst: false
  word_drop: 0.0
  # bridger configs
  map_type: att # [base,att,mat]
  head_num: 2
  share_embed: true
  tgt_scale: 0.3
  # for rnn
  mapper_type: link
  # for model loss
  layer_bow_loss: false
  share_predictor: false
  # position configs
  pos_initial: true
  use_pos_pred: false
  use_pos_exp: false
  use_st_gumbel: false
  # position model configs
  pos_type: position
  pos_feat: 2 # [1: rnn, 2: att, 3: rnn-att]
  pos_pred: 1 # [1: absolute, 2: relative]
  pos_mse: false
  pos_rank: false
  pos_dst: false
  pos_att_layer: 2
  pos_rnn_layer: 2
  # about mask
  use_enc_mask: true
  use_dec_mask: true
  # pre-train model
  pretrain_exp_dir: /mnt/cephfs_wj/bytetrans/baoyu.nlp/experiments/models/Translation/Transformer.baseline-debug
  share_encoder: false
  share_tgt_embed: false
  fine_tune: true
  pos_supervised: false
  word_supervised: false