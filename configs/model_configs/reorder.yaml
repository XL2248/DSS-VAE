model_configs:
  # model basic configs
  model_select: NonAutoReorderGEN
  task_type: Reorder
  src_max_time_step: 100
  tgt_max_time_step: 100
  cuda: true
  eval_mode: bleu
  eval_bs: 20
  dev_item: Binary_ACC
  warm_up: -1
  # encoder configs
  embed_size: &nes 512
  hidden_size: &nhs 512
  inner_size: &nis 1024
  rnn_type: gru
  enc_type: att
  enc_embed_dim: *nes
  enc_hidden_dim: *nhs
  enc_ed: &ned 0.1
  enc_rd: &nrd 0.1
  bidirectional: True
  enc_head: 8
  enc_num_layers: 4
  enc_inner_hidden: *nis
  # decoder configs
  dec_embed_dim: *nes
  dec_num_layers: 4
  dec_head: 8
  dec_hidden_dim: *nhs
  dec_inner_hidden: *nis
  dec_rd: *nrd
  dropm: *ned
  dropo: *nrd
  use_arc: false
  use_dst: false
  word_drop: 0.0
  # bridger configs
  map_type: att
  head_num: 8
  share_embed: true
  mapper_type: link
  # about position
  pos_initial: true
  use_pos_pred: true
  pos_type: position
  pos_feat: 1 # [rnn, att, rnn-att]
  pos_pred: 2 # [absolute, relative]
  pos_mse: false
  pos_rank: false
  pos_dst: true
  pos_att_layer: 3
  pos_rnn_layer: 2
  use_pos_exp: false
  use_st_gumbel: true
  # about mask
  use_enc_mask: true
  use_dec_mask: false
  # pre-train model
  pretrain_exp_dir: /mnt/cephfs_wj/bytetrans/baoyu.nlp/experiments/models/Translation/Transformer.baseline-debug
  share_encoder: false
  share_tgt_embed: false
  fine_tune: true
  pos_supervised: true
  word_supervised: false
  pos_oracle: true