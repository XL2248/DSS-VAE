data_configs:
  lang_pair: "en-de"
  train_data:
    - "/mnt/cephfs_wj/common/lab/baoyu.nlp/data/iwslt/en-de/train/train.tags.en-de.bpe.en"
    - "/mnt/cephfs_wj/common/lab/baoyu.nlp/data/iwslt/en-de/train/train.tags.en-de.bpe.de"
  valid_data:
    - "/mnt/cephfs_wj/common/lab/baoyu.nlp/data/iwslt/en-de/dev/valid.en-de.bpe.en"
    - "/mnt/cephfs_wj/common/lab/baoyu.nlp/data/iwslt/en-de/dev/valid.en-de.bpe.de"
  bleu_valid_reference: "/mnt/cephfs_wj/common/lab/baoyu.nlp/data/iwslt/en-de/dev/valid.tc.de"
  vocabularies:
    - type: "bpe"
      dict_path: "/mnt/cephfs_wj/common/lab/baoyu.nlp/data/iwslt/en-de/vocab/train.tags.en-de.bpe.en.json"
      max_n_words: 40000
    - type: "bpe"
      dict_path: "/mnt/cephfs_wj/common/lab/baoyu.nlp/data/iwslt/en-de/vocab/train.tags.en-de.bpe.de.json"
      max_n_words: 40000
  max_len:
    - -1
    - -1
  num_refs: 1
  eval_at_char_level: false

model_configs:
  model: Transformer
  n_layers: 5
  n_head: 2
  d_word_vec: 278
  d_model: 278
  d_inner_hid: 507
  dropout: 0.1
  proj_share_weight: true
  label_smoothing: 0.1

optimizer_configs:
  optimizer: "adam"
  learning_rate: 0.001
  grad_clip: -1.0
  optimizer_params: ~
  schedule_method: loss
  scheduler_configs:
    patience: 8
    min_lr: 0.0000001
#    d_model: 278
#    warmup_steps: 748

training_configs:
  seed: 1234
  max_epochs: 500000
  shuffle: true
  use_bucket: true
  batch_size: 2048
  batching_key: "tokens"
  update_cycle: 8
  valid_batch_size: 20
  disp_freq: 500
  save_freq: 500
  num_kept_checkpoints: 5
  loss_valid_freq: 500
  bleu_valid_freq: 500
  bleu_valid_batch_size: 20
  bleu_valid_warmup: 1
  bleu_valid_configs:
    max_steps: 200
    beam_size: 4
    alpha: 0.6
    postprocess: True
    length_ratio: 2.0
  early_stop_patience: 50
